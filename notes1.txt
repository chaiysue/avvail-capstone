The following questions are being evaluated as part of the peer review submission:
Are there unit tests for the API?

Are there unit tests for the model?

Are there unit tests for the logging?

Can all of the unit tests be run with a single script and do all of the unit tests pass?

Is there a mechanism to monitor performance?

Was there an attempt to isolate the read/write unit tests from production models and logs?

Does the API work as expected? For example, can you get predictions for a specific country as well as for all countries combined?

Does the data ingestion exists as a function or script to facilitate automation?

Were multiple models compared?

Did the EDA investigation use visualizations?

Is everything containerized within a working Docker image?

Did they use a visualization to compare their model to the baseline model?

Capstone Assignment 2: Through the Eyes of Our Working Example

Our Story
The business opportunity has been defined and you have come to an agreement with leadership with respect to appropriate wording and testable hypotheses. This part of the case study will focus on the next two stages in the enterprise workflow, namely data transformations and models. This part of the workflow will be treated slightly differently than previous case studies as these are time-series data, but the general procedure and best practices remain the same. These materials will provide the necessary guidance so that you can apply one of the supervised learning approaches that we have already discussed. However, you will also be encouraged to explore some of the time-series modeling tools available.



The Design Thinking Process
It is likely at this point that your original hill statements have been augmented by even more hill statements resulting from multiple playback  sessions with the client and end users. The wording and testable hypotheses you’ve agreed to would be spelled out in a set of hill  statements. A set of hill statements that all relate to one specific project or business challenge is referred to as an epic. At this point your team would likely have composed an epic made up of specific hill statements related to different aspects of the project. Capstone Part 2: Getting Started (Hands-On)
Time-series
 data are commonly encountered in data science. In particular, the field of finance, 
signal processing
, weather and other fields regularly deal with data formatted in this way. The goal of this case study is time-series forecasting, which can be thought of as the use of a model to predict future values based on previously observed values. So we could try to predict purchases, number of views or the feature most closely related to our business opportunity, revenue.

Tasks
State the different modeling approaches that you will compare to address the business opportunity.

Iterate on your suite of possible models by modifying data transformations, pipeline architectures, hyperparameters and other relevant factors.

Re-train your model on all of the data using the selected approach and prepare it for deployment.

Articulate your findings in a summary report.

Getting started
You will need to access the following link to download the files for completing this assignment prior to continuing.  Please refer to Part 2 of the Readme for additional information/instructions:

https://github.com/aavail/ai-workflow-capstone

Hint:  To standardize the approach across models ensure that your model predicts the next 30 days for any given point in time. For supervised learning, the total revenue over the next thirty days can be represented as a single number.  Capstone Part 3: Getting Started (Hands-On)
Tasks 
Build a draft version of an API with train, predict, and logfile endpoints.

Using Docker, bundle your API, model, and unit tests.

Using test-driven development iterate on your API in a way that anticipates scale, load, and drift.

Create a post-production analysis script that investigates the relationship between model performance and the business metric.

Articulate your summarized findings in a final report.

Getting started
There are three tasks in the pieces to the final part of the case study:

Ready your model for deployment.

Query your API with new data and test your monitoring tools.

Compare your results to the gold standard.

Note:  In reality your model would re-train itself at some cadence (e.g. nightly or weekly). If your systems is capable of this you should demonstrate this capacity as part of the deliverable. This is not a requirement of the model for this assignment, but it is an important aspect of model deployment to keep in mind.

You will need to access the following link to download the files for completing this assignment prior to continuing. Please refer to Part 3 of the Readme for additional information/instructions:   IBM AI Enterprise Workflow Capstone
Files for the IBM AI Enterprise Workflow Capstone project.

Part 1
Case study part 1
At this point in the project, and in any data science project really, it is best to loosly organize your code as libraries and scripts. Jupyter notebooks are a convenient and powerful tool, but we have mentioned several times that they are not a good place for source code to live. If you decide to use a notebook for this part, we recommend that it is used to run functions that live within a python module.

Deliverable goals
Overall this part of the case study is meant to tell the story of the data by investigating the relationship between the data and the business opportunity.

(1) Assimilate the business scenario and articulate testable hypotheses.

Take what you have read from the stories and from what you know about the business scenario and, in your own words, carefully re-state the business opportunity. Given the stated opportunity, enumerate the testable hypotheses.

(2) State the ideal data to address the business opportunity and clarify the rationale for needing specific data.

Note that this step is carried out before you read in the data. It helps clarify exactly what your are looking for in the data and it helps provide context for what the feature matrix and targets will look like.

Create a python script to extract relevant data from multiple data sources, automating the process of data ingestion.
From within a Python module there should be a function that reads in the data, attempts to catch common input errors and returns a feature matrix (NumPy array or Pandas DataFrame) that will subsequently be used as a starting point for EDA and modeling.

Investigate the relationship between the relevant data, the target and the business metric.
Using the feature matrix and the tools abvailable to you through EDA spend some time to get to know the data.

Articulate your findings using a deliverable with visualizations.
Summarize what you have learned in your investigations using visualizations.

Hints
The JSON files may not contain uniformly named features. Be sure to account for this in your data ingestion function.

Some of the invoice ids (invoice) have letters that can be removed to improve matching.

One common way to ready time-series data for modeling is to aggregate the transactions by day. Getting the data into this form will help you prepare for part 2.

If you have not worked with time-series or time-stamped data before the following two links can be useful.

NumPy datetime
Pandas time-series
matplotlib time-series plot
Part 2
Case study part 2
Time-series analysis is a subject area that has many varied methods and a great potential for customized solutions. We cannot cover the breadth and depth of this important area of data science in a single case study. We do however want to use this as a learning opportunity if time-series analysis is new to you. For those of you who are seasoned practitioners in this area, it may be a useful time to hone your skills or try out a more advanced technique like Gaussian processes. The reference materials for more advanced approaches to time-series analysis will occur in their own section below. If this is your first encounter with time-series data we suggest that that you begin with the supervised learning approach before trying out the other possible methods.

Deliverable goals
State the different modeling approaches that you will compare to address the business opportunity.
Iterate on your suite of possible models by modifying data transformations, pipeline architectures, hyperparameters and other relevant factors.
Re-train your model on all of the data using the selected approach and prepare it for deployment.
Articulate your findings in a summary report.
On time-series analysis
We have used TensorFlow, scikit-learn, and Spark ML as the main ways to implement models. Time-series analysis has been around a long time and there are a number of specialized packages and software to help facilitate model implementation. In the case of our business opportunity, it is required that we predict the next point or determine a reasonable value for next month's revenue. If we only had revenue, we could engineer features with revenue for the previous day, previous week, previous month and previous three months, for example. This provides features that machine learning models such as random forests or boosting could use to capture the underlying patterns or trends in the the data. You will likely spend some time optimizing this feature engineering task on a case-by-case basis.

Predicting the next element in a time-series is in line with the other machine learning tasks that we have encountered in this specialization. One caveat to this approach is that sometimes we wish to project further into the future. Although, it is not a specific request of management in the case of this business opportunity, you may want to consider forecasting multiple points into the future, say three months or more. To do this, you have two main categories of methods: 'recursive forecasting' and 'ensemble forecasting'.

In recursive forecasting, you will append your predictions to the feature matrix and roll forward until you get to the desired number of forecasts in the future. In the ensemble approach, you will use separate models for each point. It is possible to use a hybridization of these two ideas as well. If you wish to take your forecasting model to the next level, try to project several months into the future with one or both of these ideas.

Also, be aware that the assumptions of line regression are generally invalidated when using time-series data because of auto-correlation. The engineered features are derived mostly from revenue which often means that there is a high degree of correlation. You will get further with more sophisticated models to in combination with smartly engineered features.

Commonly used time-series tools
statsmodels time-series package - one of the most commonly used time-series analysis packages in Python. There are a suite of models including autoregressive models (AR), vector autoregressive models (VAR), univariate autoregressive moving average models (ARMA) and more.
Tensorflow time series tutorial
Prophet
More advanced methods for time-series analysis
PyWavelets
Bayesian Methods for time-series
Gaussian process regression
Working with time-series data
scikit-learn MultiOutputRegressor
NumPy datetime
Pandas time-series
matplotlib time-series plot
scikit-learn time-series train-test split
Additional materials
Intro paper to Gaussian Processes in time-series
Paper for using wavelets to aid time-series forecasts
Part 3
Outline
Build a draft version of an API with train, predict, and logfile endpoints.
Using Docker, bundle your API, model, and unit tests.
Using test-driven development iterate on your API in a way that anticipates scale, load, and drift.
Create a post-production analysis script that investigates the relationship between model performance and the business metric.
Articulate your summarized findings in a final report.
At a higher level you are being asked to:

Ready your model for deployment
Query your API with new data and test your monitoring tools
Compare your results to the gold standard
To ready your model for deployment you will be required to prepare you model in a way that the Flask API can both train and predict. There are some differences when you compare this model to most of those we have discussed throughout this specialization. When it comes to training one solution is that the model train script simply uses all files in a given directory. This way you could set your model up to be re-trained at regular intervals with little overhead.

Prediction in the case of this model requires a little more thought. You are not simply passing a query corresponding to a row in a feature matrix, because this business opportunity requires that the API takes a country name and a date. There are many ways to accommodate these requirements. You model may simply save the forecasts for a range of dates, then the 'predict' function serves to looks up the specified 30 day revenue prediction. You model could also transform the target date into an appropriate input vector that is then used as input for a trained model.

You might be tempted to setup the predict function to work only with the latest date, which would be appropriate in some circumstances, but in this case we are building a tool to fit the specific needs of individuals. Some people in leadership at AAVAIL make projections at the end of the month and others do this on the 15th so the predict function needs to work for all of the end users.

In the case of this project you can safely assume that there are only a few individuals that will be active users of the model so it may not be worth the effort to optimize for speed when it comes to prediction or training. The important thing is to arrive at a working solution.

Once all of your tests pass and your model is being served via Docker you will need to query the API. One suggestion for this part is to use a script to simulate the process. You may want to start with fresh log files and then for every new day make a prediction with the consideration that you have not yet seen the rest of the future data. To may the process more realistic you could 're-train' your model say every week or nightly. At a minimum you should have predictions for each day when you are finished and you should compare them to the known values.

To monitor performance there are several plots that could be made. The time-series plot where X are the day intervals and Y is the 30 day revenue (projected and known) can be particularly useful here. Because we obtain labels for y the performance of your model can be monitored by comparing predicted and known values.   



To pass the peer review for the IBM AI Enterprise Workflow Capstone, your submission must explicitly address every question in the grading rubric. Below is a comprehensive guide and code structure designed to satisfy each of the specific criteria you listed.

---

### **1. Project Directory Structure**
Organize your project exactly like this to satisfy the "Docker" and "Unit Test" requirements.

```text
aavail-capstone/
├── Dockerfile
├── requirements.txt
├── run-tests.py              # <--- Satisfies "Single script to run all tests"
├── app.py                    # <--- The API
├── cslib.py                  # <--- Common library (Data Ingestion/Model functions)
├── model.py                  # <--- Model training script
├── logger.py                 # <--- Logging logic
├── data/                     # <--- Production Data
│   └── cs-train/
├── models/                   # <--- Saved models (.joblib)
└── tests/                    # <--- Satisfies "Are there unit tests?"
    ├── test_api.py
    ├── test_model.py
    └── test_logger.py
```

---

### **2. Addressing "Data Ingestion" & "Multiple Models" (Part 1 & 2)**

**Requirement:** "Does the data ingestion exist as a function?" & "Were multiple models compared?"

In `cslib.py`:
```python
import pandas as pd
import os

def fetch_data(data_dir):
    """
    Ingests data from JSON files.
    Satisfies: 'Data ingestion exists as a function'
    """
    if not os.path.exists(data_dir):
        raise Exception("Data directory not found.")
    
    # ... logic to read json files and concatenate ...
    # Return clean DataFrame
    return df
```

In your report/notebook, you must show a chart comparing **Baseline (Average)** vs. **Random Forest**.
*   **Baseline:** Predicts the average of the last 30 days.
*   **Model:** Random Forest Regressor.
*   **Visualization:** A line chart showing Actual vs. Baseline vs. Model predictions.

---

### **3. Addressing "The API" (Part 3)**

**Requirement:** "Does the API work as expected? Can you get predictions for a specific country as well as all countries?"

You need logic in `app.py` to handle the `country` parameter.

```python
from flask import Flask, jsonify, request
import joblib
import pandas as pd
from cslib import format_input_data

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    """
    Input: {"country": "all", "year": "2019", "month": "01", "day": "05"}
    """
    if not request.json:
        return jsonify({'error': 'no request received'})
    
    data = request.json
    country = data.get("country", "all")
    
    # Load model (Ensure you have a model trained for 'all' and specific countries if needed)
    # Ideally, load a dict of models: models = {'all': model_all, 'united_kingdom': model_uk}
    model = joblib.load(f"models/sl-model-{country}.joblib") 
    
    # Prepare input vector (feature engineering)
    query_df = format_input_data(data) 
    
    prediction = model.predict(query_df)
    
    return jsonify({'prediction': prediction[0]})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

---

### **4. Addressing "Unit Tests" & "Single Script Execution"**

**Requirement:** "Can all of the unit tests be run with a single script?"

Create `run-tests.py`. This script ensures that when the reviewer runs it, they see "OK" for everything.

```python
import unittest
import sys
import os

if __name__ == '__main__':
    # Discover and run all tests in the 'tests' directory
    loader = unittest.TestLoader()
    tests = loader.discover('tests')
    testRunner = unittest.TextTestRunner()
    result = testRunner.run(tests)
    
    if not result.wasSuccessful():
        sys.exit(1)
```

**Requirement:** "Isolate read/write unit tests from production?"

In your `tests/test_model.py`, use a temporary directory or a separate `test_data` folder so you don't overwrite your real model.

```python
import unittest
import os
from model import train_model

class TestModel(unittest.TestCase):
    def test_train_model(self):
        # Use a safe path for testing
        test_model_path = "./models/test_model.joblib"
        
        # Run training on a small subset or mock data
        train_model(data_dir="./data/test_data", save_path=test_model_path)
        
        # Check if model was created
        self.assertTrue(os.path.exists(test_model_path))
        
        # Cleanup
        os.remove(test_model_path)

if __name__ == '__main__':
    unittest.main()
```

---

### **5. Addressing "Performance Monitoring"**

**Requirement:** "Is there a mechanism to monitor performance?"

You need a script (e.g., `monitoring.py`) that calculates the error (RMSE) between your predictions and the actual values once they become available.

```python
from sklearn.metrics import mean_squared_error
import numpy as np

def check_performance(actuals, predictions):
    """
    Compare predicted revenue vs actual revenue
    """
    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    print(f"Current Model RMSE: {rmse}")
    
    # Alert mechanism
    if rmse > 5000: # Threshold
        print("ALERT: Model performance has degraded. Re-training required.")
```

---

### **6. Addressing "Docker"**

**Requirement:** "Is everything containerized within a working Docker image?"

Create a `Dockerfile` in the root.

```dockerfile
# Use an official Python runtime as a parent image
FROM python:3.8-slim

# Set working directory
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Make port 8080 available to the world outside this container
EXPOSE 8080

# Run app.py when the container launches
CMD ["python", "app.py"]
```

**To test if it works (Reviewer Check):**
1.  `docker build -t aavail-api .`
2.  `docker run -p 8080:8080 aavail-api`

---

### **Checklist for Submission**

Before submitting, verify these items against the questions you provided:

*   [x] **Unit Tests:** Do you have `tests/test_api.py`, `tests/test_model.py`, `tests/test_logging.py`?
*   [x] **Single Script:** Does `python run-tests.py` run them all and pass?
*   [x] **Logging:** Does `logger.py` write to a file (e.g., `app.log`) whenever the API is called?
*   [x] **Isolation:** Do your tests write to `test.log` or `test_model.joblib` instead of the production files?
*   [x] **API:** Can you POST to `/predict` with `{"country": "United Kingdom"}` and `{"country": "all"}`?
*   [x] **Ingestion:** Do you have a `fetch_data` function in `cslib.py`?
*   [x] **Visualization:** Does your PDF report include a graph comparing the Random Forest to the Baseline?


IBM AI Enterprise Workflow Capstone
Files for the IBM AI Enterprise Workflow Capstone project.

Part 1
Case study part 1
At this point in the project, and in any data science project really, it is best to loosly organize your code as libraries and scripts. Jupyter notebooks are a convenient and powerful tool, but we have mentioned several times that they are not a good place for source code to live. If you decide to use a notebook for this part, we recommend that it is used to run functions that live within a python module.

Deliverable goals
Overall this part of the case study is meant to tell the story of the data by investigating the relationship between the data and the business opportunity.

(1) Assimilate the business scenario and articulate testable hypotheses.

Take what you have read from the stories and from what you know about the business scenario and, in your own words, carefully re-state the business opportunity. Given the stated opportunity, enumerate the testable hypotheses.

(2) State the ideal data to address the business opportunity and clarify the rationale for needing specific data.

Note that this step is carried out before you read in the data. It helps clarify exactly what your are looking for in the data and it helps provide context for what the feature matrix and targets will look like.

Create a python script to extract relevant data from multiple data sources, automating the process of data ingestion.
From within a Python module there should be a function that reads in the data, attempts to catch common input errors and returns a feature matrix (NumPy array or Pandas DataFrame) that will subsequently be used as a starting point for EDA and modeling.

Investigate the relationship between the relevant data, the target and the business metric.
Using the feature matrix and the tools abvailable to you through EDA spend some time to get to know the data.

Articulate your findings using a deliverable with visualizations.
Summarize what you have learned in your investigations using visualizations.

Hints
The JSON files may not contain uniformly named features. Be sure to account for this in your data ingestion function.

Some of the invoice ids (invoice) have letters that can be removed to improve matching.

One common way to ready time-series data for modeling is to aggregate the transactions by day. Getting the data into this form will help you prepare for part 2.

If you have not worked with time-series or time-stamped data before the following two links can be useful.

NumPy datetime
Pandas time-series
matplotlib time-series plot
Part 2
Case study part 2
Time-series analysis is a subject area that has many varied methods and a great potential for customized solutions. We cannot cover the breadth and depth of this important area of data science in a single case study. We do however want to use this as a learning opportunity if time-series analysis is new to you. For those of you who are seasoned practitioners in this area, it may be a useful time to hone your skills or try out a more advanced technique like Gaussian processes. The reference materials for more advanced approaches to time-series analysis will occur in their own section below. If this is your first encounter with time-series data we suggest that that you begin with the supervised learning approach before trying out the other possible methods.

Deliverable goals
State the different modeling approaches that you will compare to address the business opportunity.
Iterate on your suite of possible models by modifying data transformations, pipeline architectures, hyperparameters and other relevant factors.
Re-train your model on all of the data using the selected approach and prepare it for deployment.
Articulate your findings in a summary report.
On time-series analysis
We have used TensorFlow, scikit-learn, and Spark ML as the main ways to implement models. Time-series analysis has been around a long time and there are a number of specialized packages and software to help facilitate model implementation. In the case of our business opportunity, it is required that we predict the next point or determine a reasonable value for next month's revenue. If we only had revenue, we could engineer features with revenue for the previous day, previous week, previous month and previous three months, for example. This provides features that machine learning models such as random forests or boosting could use to capture the underlying patterns or trends in the the data. You will likely spend some time optimizing this feature engineering task on a case-by-case basis.

Predicting the next element in a time-series is in line with the other machine learning tasks that we have encountered in this specialization. One caveat to this approach is that sometimes we wish to project further into the future. Although, it is not a specific request of management in the case of this business opportunity, you may want to consider forecasting multiple points into the future, say three months or more. To do this, you have two main categories of methods: 'recursive forecasting' and 'ensemble forecasting'.

In recursive forecasting, you will append your predictions to the feature matrix and roll forward until you get to the desired number of forecasts in the future. In the ensemble approach, you will use separate models for each point. It is possible to use a hybridization of these two ideas as well. If you wish to take your forecasting model to the next level, try to project several months into the future with one or both of these ideas.

Also, be aware that the assumptions of line regression are generally invalidated when using time-series data because of auto-correlation. The engineered features are derived mostly from revenue which often means that there is a high degree of correlation. You will get further with more sophisticated models to in combination with smartly engineered features.

Commonly used time-series tools
statsmodels time-series package - one of the most commonly used time-series analysis packages in Python. There are a suite of models including autoregressive models (AR), vector autoregressive models (VAR), univariate autoregressive moving average models (ARMA) and more.
Tensorflow time series tutorial
Prophet
More advanced methods for time-series analysis
PyWavelets
Bayesian Methods for time-series
Gaussian process regression
Working with time-series data
scikit-learn MultiOutputRegressor
NumPy datetime
Pandas time-series
matplotlib time-series plot
scikit-learn time-series train-test split
Additional materials
Intro paper to Gaussian Processes in time-series
Paper for using wavelets to aid time-series forecasts
Part 3
Outline
Build a draft version of an API with train, predict, and logfile endpoints.
Using Docker, bundle your API, model, and unit tests.
Using test-driven development iterate on your API in a way that anticipates scale, load, and drift.
Create a post-production analysis script that investigates the relationship between model performance and the business metric.
Articulate your summarized findings in a final report.
At a higher level you are being asked to:

Ready your model for deployment
Query your API with new data and test your monitoring tools
Compare your results to the gold standard
To ready your model for deployment you will be required to prepare you model in a way that the Flask API can both train and predict. There are some differences when you compare this model to most of those we have discussed throughout this specialization. When it comes to training one solution is that the model train script simply uses all files in a given directory. This way you could set your model up to be re-trained at regular intervals with little overhead.

Prediction in the case of this model requires a little more thought. You are not simply passing a query corresponding to a row in a feature matrix, because this business opportunity requires that the API takes a country name and a date. There are many ways to accommodate these requirements. You model may simply save the forecasts for a range of dates, then the 'predict' function serves to looks up the specified 30 day revenue prediction. You model could also transform the target date into an appropriate input vector that is then used as input for a trained model.

You might be tempted to setup the predict function to work only with the latest date, which would be appropriate in some circumstances, but in this case we are building a tool to fit the specific needs of individuals. Some people in leadership at AAVAIL make projections at the end of the month and others do this on the 15th so the predict function needs to work for all of the end users.

In the case of this project you can safely assume that there are only a few individuals that will be active users of the model so it may not be worth the effort to optimize for speed when it comes to prediction or training. The important thing is to arrive at a working solution.

Once all of your tests pass and your model is being served via Docker you will need to query the API. One suggestion for this part is to use a script to simulate the process. You may want to start with fresh log files and then for every new day make a prediction with the consideration that you have not yet seen the rest of the future data. To may the process more realistic you could 're-train' your model say every week or nightly. At a minimum you should have predictions for each day when you are finished and you should compare them to the known values.

To monitor performance there are several plots that could be made. The time-series plot where X are the day intervals and Y is the 30 day revenue (projected and known) can be particularly useful here. Because we obtain labels for y the performance of your model can be monitored by comparing predicted and known values.


